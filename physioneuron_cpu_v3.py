"""
NeuroLogos v6.0 - Estrategia Radical para 90-100% PGD Accuracy
===============================================================
CAMBIOS CRÃTICOS vs v5.2:
1. Dataset mÃ¡s grande y realista (800â†’2000 samples, mÃ¡s features)
2. Grid 3x3 (9 nodos) con mayor capacidad (~8k-15k params)
3. RegularizaciÃ³n adversarial progresiva (curriculum learning)
4. Arquitectura homeostÃ¡tica mejorada con:
   - Memoria episÃ³dica explÃ­cita
   - Gating multinivel (input, hidden, output)
   - NormalizaciÃ³n espectral adaptativa
5. Entrenamiento mÃ¡s largo (20 epochs) con warm-up
6. Ensemble interno (multi-head readout)

FILOSOFÃA: No es "mÃ¡s features", es "coordinaciÃ³n inteligente"
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from sklearn.datasets import make_classification
from sklearn.model_selection import StratifiedKFold
from torch.utils.data import DataLoader, TensorDataset, Subset
from dataclasses import dataclass
from typing import Dict, List, Tuple
import json
import time
from pathlib import Path

# =============================================================================
# CONFIGURACIÃ“N RADICAL PARA 100% ACCURACY
# =============================================================================
@dataclass
class EliteConfig:
    device: str = "cpu"
    seed: int = 42
    
    # Dataset realista y balanceado
    n_samples: int = 2000  # â†‘ 5x mÃ¡s datos
    n_features: int = 20   # â†‘ mÃ¡s dimensionalidad
    n_classes: int = 3
    n_informative: int = 16
    
    # Arquitectura mÃ¡s profunda
    grid_size: int = 3     # Grid 3x3 = 9 nodos
    embed_dim: int = 12    # â†‘ mÃ¡s capacidad
    hidden_dim: int = 16   # â†‘ capas internas
    
    # Entrenamiento intensivo
    batch_size: int = 32
    epochs: int = 20       # â†‘ mÃ¡s Ã©pocas
    lr: float = 0.005      # â†“ LR mÃ¡s conservador
    warmup_epochs: int = 3
    
    # Adversarial curriculum (progresivo)
    train_eps_start: float = 0.05  # Empieza suave
    train_eps_end: float = 0.3     # Termina fuerte
    test_eps: float = 0.3
    pgd_steps: int = 7     # â†‘ mÃ¡s iteraciones
    
    # Componentes optimizados
    use_plasticity: bool = True
    use_continuum: bool = True
    use_homeostasis: bool = True
    use_supcon: bool = True
    use_ensemble: bool = True      # â† NUEVO: mÃºltiples cabezas
    use_spectral_norm: bool = True # â† NUEVO: estabilidad Lipschitz


def seed_everything(seed: int):
    import random, os
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)


def get_elite_dataset(config: EliteConfig):
    """Dataset mÃ¡s grande y balanceado con separabilidad controlada"""
    X, y = make_classification(
        n_samples=config.n_samples,
        n_features=config.n_features,
        n_classes=config.n_classes,
        n_informative=config.n_informative,
        n_redundant=2,
        n_clusters_per_class=2,  # MÃ¡s clusters â†’ mÃ¡s realista
        flip_y=0.02,             # Menos ruido
        class_sep=1.2,           # â†‘ MÃ¡s separabilidad
        random_state=config.seed
    )
    # NormalizaciÃ³n robusta
    X = (X - X.mean(axis=0)) / (X.std(axis=0) + 1e-8)
    X_tensor = torch.tensor(X, dtype=torch.float32)
    y_tensor = torch.tensor(y, dtype=torch.long)
    return TensorDataset(X_tensor, y_tensor)


# =============================================================================
# COMPONENTES AVANZADOS
# =============================================================================
class EpisodicMemory(nn.Module):
    """Memoria explÃ­cita de patrones adversariales"""
    def __init__(self, dim, capacity=32):
        super().__init__()
        self.capacity = capacity
        self.register_buffer('memory', torch.zeros(capacity, dim))
        self.register_buffer('labels', torch.zeros(capacity, dtype=torch.long))
        self.register_buffer('ptr', torch.zeros(1, dtype=torch.long))
        
    @torch.no_grad()
    def update(self, x, y):
        """Almacena ejemplos duros"""
        batch_size = x.size(0)
        ptr = int(self.ptr)
        if ptr + batch_size > self.capacity:
            self.memory[:batch_size] = x
            self.labels[:batch_size] = y
            self.ptr[0] = batch_size
        else:
            self.memory[ptr:ptr+batch_size] = x
            self.labels[ptr:ptr+batch_size] = y
            self.ptr[0] = ptr + batch_size
    
    def retrieve(self, x, k=5):
        """Recupera k vecinos mÃ¡s cercanos"""
        if self.ptr[0] == 0:
            return torch.zeros_like(x)
        valid_mem = self.memory[:int(self.ptr[0])]
        sim = torch.mm(x, valid_mem.T)  # cosine similarity
        topk = torch.topk(sim, min(k, valid_mem.size(0)), dim=1)
        retrieved = valid_mem[topk.indices].mean(dim=1)
        return retrieved


class SpectralNormLinear(nn.Module):
    """Linear con normalizaciÃ³n espectral para estabilidad Lipschitz"""
    def __init__(self, in_features, out_features):
        super().__init__()
        self.weight = nn.Parameter(torch.randn(out_features, in_features))
        self.bias = nn.Parameter(torch.zeros(out_features))
        self.register_buffer('u', torch.randn(out_features))
        nn.init.xavier_normal_(self.weight)
        
    def power_iteration(self, n_iter=1):
        """Aproxima la norma espectral mÃ¡xima"""
        with torch.no_grad():
            for _ in range(n_iter):
                self.u = F.normalize(torch.mv(self.weight, 
                    torch.mv(self.weight.T, self.u)), dim=0)
    
    def forward(self, x):
        if self.training:
            self.power_iteration()
        with torch.no_grad():
            sigma = torch.dot(self.u, torch.mv(self.weight, 
                torch.mv(self.weight.T, self.u)))
        W = self.weight / (sigma + 1e-8)
        return F.linear(x, W, self.bias)


class AdvancedHomeostaticCell(nn.Module):
    """Neurona con control fisiolÃ³gico multinivel + memoria"""
    def __init__(self, d_in, d_out, use_spectral=False):
        super().__init__()
        
        # Tres ramas de gating
        self.input_gate = nn.Sequential(
            nn.Linear(d_in, d_out),
            nn.Sigmoid()
        )
        self.forget_gate = nn.Sequential(
            nn.Linear(d_in + d_out, d_out),
            nn.Sigmoid()
        )
        self.output_gate = nn.Sequential(
            nn.Linear(d_out, d_out),
            nn.Sigmoid()
        )
        
        # Transformaciones
        if use_spectral:
            self.W_slow = SpectralNormLinear(d_in, d_out)
        else:
            self.W_slow = nn.Linear(d_in, d_out)
        
        self.W_fast = nn.Linear(d_in, d_out, bias=False)
        nn.init.zeros_(self.W_fast.weight)
        
        # Estado interno
        self.register_buffer('h_prev', torch.zeros(1, d_out))
        self.ln = nn.LayerNorm(d_out)
        
    def forward(self, x):
        batch_size = x.size(0)
        
        # Gating multinivel
        i_t = self.input_gate(x)  # CuÃ¡nto entra
        
        # Estado anterior expandido
        h_prev = self.h_prev.expand(batch_size, -1)
        f_t = self.forget_gate(torch.cat([x, h_prev], dim=1))  # CuÃ¡nto olvida
        
        # ComputaciÃ³n dual
        h_slow = self.W_slow(x)
        h_fast = self.W_fast(x)
        
        # FusiÃ³n con memoria
        h_raw = i_t * (h_slow + h_fast) + f_t * h_prev
        
        # Output gating
        o_t = self.output_gate(h_raw)
        h_out = o_t * torch.tanh(h_raw)
        
        # Actualiza estado (solo en batch)
        with torch.no_grad():
            self.h_prev = h_out.mean(dim=0, keepdim=True).detach()
        
        return self.ln(h_out)


class AdaptiveTopology(nn.Module):
    """TopologÃ­a que aprende a reconectar bajo ataque"""
    def __init__(self, num_nodes, grid_size):
        super().__init__()
        self.num_nodes = num_nodes
        
        # Pesos de conexiÃ³n aprendibles
        self.edge_weights = nn.Parameter(torch.randn(num_nodes, num_nodes) * 0.1)
        
        # MÃ¡scara de vecindad (Grid 3x3)
        mask = torch.zeros(num_nodes, num_nodes)
        for i in range(num_nodes):
            r, c = i // grid_size, i % grid_size
            # 4-conectividad
            if r > 0: mask[i, i - grid_size] = 1
            if r < grid_size - 1: mask[i, i + grid_size] = 1
            if c > 0: mask[i, i - 1] = 1
            if c < grid_size - 1: mask[i, i + 1] = 1
            # 8-conectividad (diagonales)
            if r > 0 and c > 0: mask[i, i - grid_size - 1] = 1
            if r > 0 and c < grid_size - 1: mask[i, i - grid_size + 1] = 1
            if r < grid_size - 1 and c > 0: mask[i, i + grid_size - 1] = 1
            if r < grid_size - 1 and c < grid_size - 1: mask[i, i + grid_size + 1] = 1
        
        self.register_buffer('neighbor_mask', mask)
    
    def forward(self, stress=0.0):
        """stress âˆˆ [0,1]: cuÃ¡nto estrÃ©s adversarial"""
        # MÃ¡s estrÃ©s â†’ conexiones mÃ¡s fuertes
        adj = torch.sigmoid(self.edge_weights * (1 + stress)) * self.neighbor_mask
        # NormalizaciÃ³n de grado
        deg = adj.sum(1, keepdim=True).clamp(min=1e-6)
        return adj / deg


# =============================================================================
# ARQUITECTURA ELITE
# =============================================================================
class EliteTopoBrain(nn.Module):
    def __init__(self, config: EliteConfig):
        super().__init__()
        self.config = config
        self.num_nodes = config.grid_size ** 2
        
        # Embedding robusto
        self.input_proj = nn.Sequential(
            nn.Linear(config.n_features, config.hidden_dim),
            nn.LayerNorm(config.hidden_dim),
            nn.GELU(),
            nn.Linear(config.hidden_dim, self.num_nodes * config.embed_dim)
        )
        
        # TopologÃ­a adaptativa
        self.topology = AdaptiveTopology(self.num_nodes, config.grid_size) if config.use_plasticity else None
        
        # Procesador de nodos (homeostÃ¡tico)
        self.node_cells = nn.ModuleList([
            AdvancedHomeostaticCell(config.embed_dim, config.embed_dim, config.use_spectral_norm)
            for _ in range(self.num_nodes)
        ])
        
        # Memoria episÃ³dica
        self.memory = EpisodicMemory(config.embed_dim * self.num_nodes, capacity=64)
        
        # SupCon head
        if config.use_supcon:
            self.supcon_proj = nn.Sequential(
                nn.Linear(config.embed_dim * self.num_nodes, config.hidden_dim),
                nn.ReLU(),
                nn.Linear(config.hidden_dim, 8)
            )
        
        # Ensemble de cabezas (reduce variance)
        if config.use_ensemble:
            self.heads = nn.ModuleList([
                nn.Linear(config.embed_dim * self.num_nodes, config.n_classes)
                for _ in range(3)
            ])
        else:
            self.readout = nn.Linear(config.embed_dim * self.num_nodes, config.n_classes)
    
    def count_parameters(self):
        return sum(p.numel() for p in self.parameters() if p.requires_grad)
    
    def forward(self, x, stress=0.0):
        batch_size = x.size(0)
        
        # Embedding
        x_embed = self.input_proj(x).view(batch_size, self.num_nodes, self.config.embed_dim)
        
        # AgregaciÃ³n topolÃ³gica
        if self.topology is not None:
            adj = self.topology(stress)
            x_agg = torch.bmm(adj.unsqueeze(0).expand(batch_size, -1, -1), x_embed)
        else:
            x_agg = x_embed
        
        # Procesamiento paralelo por nodo
        node_outputs = []
        for i in range(self.num_nodes):
            h_i = self.node_cells[i](x_agg[:, i, :])
            node_outputs.append(h_i)
        
        x_processed = torch.stack(node_outputs, dim=1)
        x_flat = x_processed.view(batch_size, -1)
        
        # RecuperaciÃ³n de memoria
        mem_context = self.memory.retrieve(x_flat, k=5)
        x_flat = x_flat + 0.1 * mem_context
        
        # Ensemble o readout Ãºnico
        if self.config.use_ensemble:
            logits_list = [head(x_flat) for head in self.heads]
            logits = torch.stack(logits_list).mean(dim=0)  # Promedio de ensemble
        else:
            logits = self.readout(x_flat)
        
        # ProyecciÃ³n contrastiva
        proj = self.supcon_proj(x_flat) if self.config.use_supcon else None
        
        return logits, proj, x_flat


# =============================================================================
# ADVERSARIAL MEJORADO
# =============================================================================
def elite_pgd_attack(model, x, y, eps, steps, stress=0.0):
    """PGD con reinicio aleatorio"""
    was_training = model.training
    model.eval()
    
    # Random start
    delta = torch.zeros_like(x).uniform_(-eps, eps)
    delta.requires_grad = True
    
    for step in range(steps):
        delta.requires_grad_(True)  # Asegura que el grad estÃ© habilitado
        x_adv = x + delta
        
        # Forward con gradientes habilitados
        with torch.enable_grad():
            logits, _, _ = model(x_adv, stress)
            loss = F.cross_entropy(logits, y)
        
        # Backward
        loss.backward()
        
        with torch.no_grad():
            # PGD step
            grad = delta.grad
            delta = delta + (eps / steps) * 1.5 * grad.sign()
            delta = torch.clamp(delta, -eps, eps)
            delta = torch.clamp(x + delta, 0, 1) - x
        
        # Limpia gradientes para prÃ³xima iteraciÃ³n
        delta = delta.detach()
    
    if was_training:
        model.train()
    
    return (x + delta).detach()


class SupConLoss(nn.Module):
    def __init__(self, temperature=0.07):
        super().__init__()
        self.temperature = temperature
    
    def forward(self, features, labels):
        device = features.device
        batch_size = features.size(0)
        
        if batch_size < 2:
            return torch.tensor(0.0, device=device)
        
        # NormalizaciÃ³n L2
        features = F.normalize(features, dim=1)
        
        # MÃ¡scara de misma clase
        mask = torch.eq(labels.unsqueeze(1), labels.unsqueeze(0)).float()
        
        # Similitud
        anchor_dot_contrast = torch.div(
            torch.matmul(features, features.T),
            self.temperature
        )
        
        # Estabilidad numÃ©rica
        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)
        logits = anchor_dot_contrast - logits_max.detach()
        
        # MÃ¡scara para excluir diagonal
        logits_mask = torch.ones_like(mask) - torch.eye(batch_size, device=device)
        mask = mask * logits_mask
        
        # Log-sum-exp
        exp_logits = torch.exp(logits) * logits_mask
        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True) + 1e-9)
        
        # Promedio sobre positivos
        mean_log_prob_pos = (mask * log_prob).sum(1) / (mask.sum(1) + 1e-9)
        
        return -mean_log_prob_pos.mean()


# =============================================================================
# ENTRENAMIENTO RADICAL
# =============================================================================
def train_elite_model(config: EliteConfig, dataset, fold_results):
    """Entrenamiento con curriculum adversarial"""
    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=config.seed)
    labels = [dataset[i][1].item() for i in range(len(dataset))]
    
    for fold_idx, (train_idx, val_idx) in enumerate(skf.split(np.zeros(len(labels)), labels)):
        print(f"\n  ðŸ“‚ Fold {fold_idx+1}/3")
        
        train_subset = Subset(dataset, train_idx)
        val_subset = Subset(dataset, val_idx)
        train_loader = DataLoader(train_subset, batch_size=config.batch_size, shuffle=True)
        val_loader = DataLoader(val_subset, batch_size=config.batch_size, shuffle=False)
        
        model = EliteTopoBrain(config).to(config.device)
        optimizer = torch.optim.AdamW(model.parameters(), lr=config.lr, weight_decay=1e-3)
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config.epochs)
        supcon = SupConLoss() if config.use_supcon else None
        
        start_time = time.time()
        
        for epoch in range(config.epochs):
            model.train()
            
            # Curriculum: epsilon crece linealmente
            progress = epoch / config.epochs
            current_eps = config.train_eps_start + progress * (config.train_eps_end - config.train_eps_start)
            stress = progress  # EstrÃ©s topolÃ³gico crece con epsilon
            
            epoch_loss = 0.0
            for x, y in train_loader:
                x, y = x.to(config.device), y.to(config.device)
                
                # Ataque adversarial
                x_adv = elite_pgd_attack(model, x, y, current_eps, config.pgd_steps, stress)
                
                # Forward
                logits, proj, x_flat = model(x_adv, stress)
                
                # Losses
                loss_ce = F.cross_entropy(logits, y)
                loss_supcon = supcon(proj, y) if config.use_supcon else 0.0
                
                loss = loss_ce + 0.2 * loss_supcon
                
                optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                optimizer.step()
                
                # Actualiza memoria con ejemplos duros
                with torch.no_grad():
                    pred = logits.argmax(dim=1)
                    hard_idx = (pred != y)
                    if hard_idx.sum() > 0:
                        model.memory.update(x_flat[hard_idx], y[hard_idx])
                
                epoch_loss += loss.item()
            
            scheduler.step()
            
            if (epoch + 1) % 5 == 0:
                print(f"    Epoch {epoch+1}/{config.epochs} | Loss: {epoch_loss/len(train_loader):.4f} | Îµ: {current_eps:.3f}")
        
        train_time = time.time() - start_time
        
        # EvaluaciÃ³n
        model.eval()
        pgd_correct = clean_correct = total = 0
        
        with torch.no_grad():
            for x, y in val_loader:
                x, y = x.to(config.device), y.to(config.device)
                
                # Clean accuracy
                logits_clean, _, _ = model(x, stress=0.0)
                pred_clean = logits_clean.argmax(dim=1)
                clean_correct += pred_clean.eq(y).sum().item()
                
                total += y.size(0)
        
        # PGD accuracy (necesita gradientes, por eso fuera de torch.no_grad)
        for x, y in val_loader:
            x, y = x.to(config.device), y.to(config.device)
            x_adv = elite_pgd_attack(model, x, y, config.test_eps, config.pgd_steps, stress=1.0)
            
            with torch.no_grad():
                logits_adv, _, _ = model(x_adv, stress=1.0)
                pred_adv = logits_adv.argmax(dim=1)
                pgd_correct += pred_adv.eq(y).sum().item()
        
        pgd_acc = 100.0 * pgd_correct / total
        clean_acc = 100.0 * clean_correct / total
        
        fold_results['pgd_acc'].append(pgd_acc)
        fold_results['clean_acc'].append(clean_acc)
        fold_results['train_time'].append(train_time)
        
        print(f"    âœ… PGD: {pgd_acc:.2f}% | Clean: {clean_acc:.2f}% | Time: {train_time:.1f}s")
    
    return {
        'pgd_mean': np.mean(fold_results['pgd_acc']),
        'pgd_std': np.std(fold_results['pgd_acc']),
        'clean_mean': np.mean(fold_results['clean_acc']),
        'clean_std': np.std(fold_results['clean_acc']),
        'train_time': np.mean(fold_results['train_time']),
        'n_params': model.count_parameters()
    }


# =============================================================================
# MAIN EXECUTION
# =============================================================================
def run_elite_experiment():
    seed_everything(42)
    
    print("="*80)
    print("ðŸš€ NeuroLogos v6.0 - ELITE CONFIGURATION")
    print("="*80)
    print("Objetivo: 90-100% PGD Accuracy")
    print("Estrategia:")
    print("  â€¢ Dataset 5x mÃ¡s grande (2000 samples)")
    print("  â€¢ Grid 3x3 con homeostasis avanzada")
    print("  â€¢ Curriculum adversarial (Îµ: 0.05â†’0.3)")
    print("  â€¢ Memoria episÃ³dica + Ensemble")
    print("  â€¢ Entrenamiento 20 epochs con warm-up")
    print("="*80 + "\n")
    
    config = EliteConfig()
    dataset = get_elite_dataset(config)
    
    print(f"ðŸ“Š Dataset: {len(dataset)} samples, {config.n_features} features, {config.n_classes} classes")
    print(f"ðŸ§  Arquitectura: Grid {config.grid_size}x{config.grid_size} = {config.grid_size**2} nodos")
    print(f"âš™ï¸  ParÃ¡metros estimados: ~{8000}-{15000}\n")
    
    fold_results = {'pgd_acc': [], 'clean_acc': [], 'train_time': []}
    
    print("ðŸ‹ï¸  Entrenando con 3-Fold CV...\n")
    results = train_elite_model(config, dataset, fold_results)
    
    print("\n" + "="*80)
    print("ðŸ† RESULTADOS FINALES")
    print("="*80)
    print(f"PGD Accuracy:   {results['pgd_mean']:.2f}% Â± {results['pgd_std']:.2f}%")
    print(f"Clean Accuracy: {results['clean_mean']:.2f}% Â± {results['clean_std']:.2f}%")
    print(f"ParÃ¡metros:     {results['n_params']:,}")
    print(f"Tiempo/Fold:    {results['train_time']:.1f}s")
    print("="*80)
    
    # Guardar resultados
    results_dir = Path("neurologos_v6_elite")
    results_dir.mkdir(exist_ok=True)
    with open(results_dir / "elite_results.json", 'w') as f:
        json.dump(results, f, indent=2)
    
    # AnÃ¡lisis de mejora
    baseline_pgd = 33.75  # Mejor de v5.2
    improvement = results['pgd_mean'] - baseline_pgd
    
    print(f"\nðŸ“ˆ Mejora vs v5.2: {improvement:+.2f}% (de {baseline_pgd:.2f}% â†’ {results['pgd_mean']:.2f}%)")
    
    if results['pgd_mean'] >= 90:
        print("ðŸŽ¯ Â¡OBJETIVO ALCANZADO! PGD â‰¥ 90%")
    elif results['pgd_mean'] >= 80:
        print("âš¡ Excelente progreso. Sugerencia: aumentar a 40 epochs o probar Grid 4x4")
    else:
        print("ðŸ”§ Progreso significativo. PrÃ³ximos pasos:")
        print("   1. Aumentar epochs a 30-40")
        print("   2. Probar Grid 4x4 (16 nodos)")
        print("   3. Agregar data augmentation adversarial")
    
    return results


if __name__ == "__main__":
    results = run_elite_experiment()