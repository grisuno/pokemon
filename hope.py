import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from torch.utils.data import DataLoader, TensorDataset
from typing import Dict, List, Optional, Tuple
import time
from collections import defaultdict
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split

# ========================================================================
# CONFIGURACI√ìN REALISTA
# ========================================================================

class Config:
    """Configuraci√≥n para desaf√≠o realista con adversarial"""
    SEED = 42
    NUM_EPOCHS = 50  # M√°s √©pocas para ver evoluci√≥n
    BATCH_SIZE = 32
    LEARNING_RATE = 5e-3
    WEIGHT_DECAY = 1e-3
    
    # Arquitectura balanceada
    D_MODEL = 96
    MLP_HIDDEN = 192
    
    # CMS con 3 niveles
    CMS_FREQUENCIES = [1, 4, 16]
    
    # Adversarial training (curriculum)
    ADV_EPSILON_START = 0.05
    ADV_EPSILON_END = 0.3
    ADV_STEPS = 5
    
    # Ablaci√≥n
    ENABLE_HOMEOSTATIC = True
    ENABLE_CMS = True
    ENABLE_ADVERSARIAL = True

def setup_device():
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    if device.type == 'cuda':
        print(f"‚úÖ GPU: {torch.cuda.get_device_name(0)}")
    else:
        print("‚ö†Ô∏è  CPU mode")
    return device

def set_seed(seed: int):
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)

# ========================================================================
# DATASET REAL (Digits con Concept Drift)
# ========================================================================

class RealWorldEnvironment:
    """
    Dataset Digits con separaci√≥n en "mundos" para simular concept drift
    Similar al segundo ejemplo pero adaptado para classification
    """
    
    def __init__(self, seed=42):
        # Cargar Digits (8x8 = 64 features, 10 clases)
        data, target = load_digits(return_X_y=True)
        data = data / 16.0  # Normalizar [0, 1]
        
        # Split train/test general
        X_train, X_test, y_train, y_test = train_test_split(
            data, target, test_size=0.2, random_state=seed, stratify=target
        )
        
        self.X_train = torch.tensor(X_train, dtype=torch.float32)
        self.y_train = torch.tensor(y_train, dtype=torch.long)
        self.X_test = torch.tensor(X_test, dtype=torch.float32)
        self.y_test = torch.tensor(y_test, dtype=torch.long)
        
        # MUNDO 1: D√≠gitos 0-4 (train)
        mask1 = self.y_train < 5
        self.X1_train = self.X_train[mask1]
        self.y1_train = self.y_train[mask1]
        
        # MUNDO 2: D√≠gitos 5-9 (train)
        mask2 = self.y_train >= 5
        self.X2_train = self.X_train[mask2]
        self.y2_train = self.y_train[mask2]
        
        # Test completo
        self.X_all_test = self.X_test
        self.y_all_test = self.y_test
        
        print(f"üìä Dataset Digits:")
        print(f"   World 1 (0-4): {len(self.X1_train)} samples")
        print(f"   World 2 (5-9): {len(self.X2_train)} samples")
        print(f"   Test: {len(self.X_test)} samples")
    
    def get_batch(self, phase: str, batch_size: int = 32):
        """Obtener batch seg√∫n la fase de entrenamiento"""
        if phase == "WORLD_1":
            idx = torch.randint(0, len(self.X1_train), (batch_size,))
            return self.X1_train[idx], self.y1_train[idx]
        
        elif phase == "WORLD_2":
            idx = torch.randint(0, len(self.X2_train), (batch_size,))
            return self.X2_train[idx], self.y2_train[idx]
        
        elif phase == "CHAOS":
            # Mezcla + ruido
            idx = torch.randint(0, len(self.X_train), (batch_size,))
            noise = torch.randn_like(self.X_train[idx]) * 0.3
            return self.X_train[idx] + noise, self.y_train[idx]
        
        else:  # MIXED
            idx = torch.randint(0, len(self.X_train), (batch_size,))
            return self.X_train[idx], self.y_train[idx]
    
    def get_test_loader(self, batch_size: int = 32):
        """Test loader completo"""
        dataset = TensorDataset(self.X_all_test, self.y_all_test)
        return DataLoader(dataset, batch_size=batch_size, shuffle=False)

# ========================================================================
# REGULADOR HOMEOST√ÅTICO
# ========================================================================

class HomeostaticRegulator(nn.Module):
    """Motor fisiol√≥gico que regula seg√∫n estado interno"""
    
    def __init__(self, d_model: int):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(3, 16),
            nn.LayerNorm(16),
            nn.Tanh(),
            nn.Linear(16, 3),
            nn.Sigmoid()
        )
        
    def forward(self, x: torch.Tensor, h_prev: torch.Tensor, w_norm: float) -> Dict:
        """
        Calcula controles homeost√°ticos basados en:
        - Estr√©s (varianza input)
        - Excitaci√≥n (magnitud activaci√≥n)
        - Fatiga (norma pesos)
        """
        B, D = x.shape
        
        # Sensor 1: Estr√©s
        stress = (x.var(dim=1, keepdim=True) - 0.5).abs()
        
        # Sensor 2: Excitaci√≥n
        excitation = h_prev.abs().mean(dim=1, keepdim=True)
        
        # Sensor 3: Fatiga
        fatigue = torch.full((B, 1), w_norm / 10.0, device=x.device)
        
        # Fusi√≥n
        state = torch.cat([stress, excitation, fatigue], dim=-1)
        controls = self.net(state)
        
        return {
            'metabolism': controls[:, 0:1],   # Tasa de aprendizaje
            'sensitivity': controls[:, 1:2],  # Pendiente activaci√≥n
            'gate': controls[:, 2:3]          # Mezcla fast/slow
        }

# ========================================================================
# MEMORIA L√çQUIDA EFICIENTE
# ========================================================================

class LiquidMemory(nn.Module):
    """Memoria l√≠quida con componente r√°pida y lenta"""
    
    def __init__(self, d_model: int):
        super().__init__()
        self.d_model = d_model
        
        # Memoria lenta (par√°metro entrenable)
        self.memory_slow = nn.Parameter(torch.randn(d_model) * 0.01)
        
        # Proyecci√≥n r√°pida
        self.fast_proj = nn.Linear(d_model, d_model, bias=False)
        nn.init.orthogonal_(self.fast_proj.weight, gain=1.0)
        
    def forward(self, x: torch.Tensor, physio: Dict) -> torch.Tensor:
        """
        Args:
            x: (B, D)
            physio: Controles homeost√°ticos
        """
        # Fast component
        fast = self.fast_proj(x)
        
        # Slow component (broadcast)
        slow = self.memory_slow.unsqueeze(0).expand_as(x)
        
        # Mezcla con gate
        output = fast + physio['gate'] * slow
        
        return output

# ========================================================================
# SELF-MODIFYING MEMORY
# ========================================================================

class EfficientSelfModMemory(nn.Module):
    """Self-modifying memory con homeostasis"""
    
    def __init__(self, d_model: int, hidden_dim: int):
        super().__init__()
        self.d_model = d_model
        
        # Proyecciones
        self.proj_kv = nn.Linear(d_model, d_model * 2)
        self.proj_q = nn.Linear(d_model, d_model)
        
        # Memoria l√≠quida
        self.liquid_mem = LiquidMemory(d_model)
        
        # Motor homeost√°tico
        self.regulator = HomeostaticRegulator(d_model)
        
        # Output
        self.output_proj = nn.Sequential(
            nn.Linear(d_model, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, d_model),
            nn.LayerNorm(d_model)
        )
        
        # Estado previo
        self.register_buffer('prev_activation', None)
        
    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Args:
            x: (B, D)
        Returns:
            output, h_current
        """
        B, D = x.shape
        
        if self.prev_activation is None or self.prev_activation.shape[0] != B:
            self.prev_activation = torch.zeros_like(x)
        
        # Proyecciones
        kv = self.proj_kv(x)
        k, v = kv.chunk(2, dim=-1)
        q = self.proj_q(x)
        
        # Norma de pesos
        with torch.no_grad():
            w_norm = self.proj_kv.weight.norm().item()
        
        # Controles homeost√°ticos
        physio = self.regulator(x, self.prev_activation, w_norm)
        
        # Memoria l√≠quida
        mem_out = self.liquid_mem(v, physio)
        
        # Atenci√≥n simplificada
        attn = torch.sum(q * k, dim=-1, keepdim=True) / (D ** 0.5)
        attn = torch.sigmoid(attn)
        
        # Combinar
        combined = attn * mem_out + (1 - attn) * v
        
        # Activaci√≥n sensible (Swish din√°mico)
        beta = 0.5 + physio['sensitivity'] * 2.0
        activated = combined * torch.sigmoid(beta * combined)
        
        # Output
        output = self.output_proj(activated)
        
        # Actualizar estado
        with torch.no_grad():
            self.prev_activation = activated.detach()
        
        return output, activated

# ========================================================================
# CMS (ARREGLADO)
# ========================================================================

class ContinuumMemorySystem(nn.Module):
    """CMS que acepta global_step correctamente"""
    
    def __init__(self, frequencies: List[int], d_model: int, hidden_dim: int):
        super().__init__()
        self.frequencies = frequencies
        
        self.levels = nn.ModuleList([
            nn.Sequential(
                nn.Linear(d_model, hidden_dim),
                nn.ReLU(),
                nn.Dropout(0.1),
                nn.Linear(hidden_dim, d_model),
                nn.LayerNorm(d_model)
            ) for _ in frequencies
        ])
    
    def forward(self, x: torch.Tensor, global_step: int = 0) -> torch.Tensor:
        """
        Args:
            x: (B, D)
            global_step: Paso global
        """
        out = x
        for level, freq in zip(self.levels, self.frequencies):
            if global_step % freq == 0:
                out = out + level(out)
        return out

# ========================================================================
# MODELO HOPE + PHYSIO
# ========================================================================

class HopePhysioModel(nn.Module):
    """Hope + PhysioChimera para clasificaci√≥n"""
    
    def __init__(self, config: Config, n_features: int, n_classes: int):
        super().__init__()
        self.config = config
        
        # Input projection
        self.input_proj = nn.Sequential(
            nn.Linear(n_features, config.MLP_HIDDEN),
            nn.LayerNorm(config.MLP_HIDDEN),
            nn.GELU(),
            nn.Linear(config.MLP_HIDDEN, config.D_MODEL)
        )
        
        # Self-Modifying con Homeostasis
        if config.ENABLE_HOMEOSTATIC:
            self.memory = EfficientSelfModMemory(config.D_MODEL, config.MLP_HIDDEN)
        else:
            self.memory = nn.Sequential(
                nn.Linear(config.D_MODEL, config.MLP_HIDDEN),
                nn.ReLU(),
                nn.Dropout(0.1),
                nn.Linear(config.MLP_HIDDEN, config.D_MODEL),
                nn.LayerNorm(config.D_MODEL)
            )
        
        # CMS
        if config.ENABLE_CMS:
            self.cms = ContinuumMemorySystem(
                config.CMS_FREQUENCIES,
                config.D_MODEL,
                config.MLP_HIDDEN
            )
        else:
            # Identity que acepta global_step
            self.cms = lambda x, global_step: x
        
        # Output
        self.output_proj = nn.Linear(config.D_MODEL, n_classes)
        
    def reset_states(self):
        if hasattr(self.memory, 'prev_activation'):
            self.memory.prev_activation = None
    
    def forward(self, x: torch.Tensor, global_step: int = 0) -> torch.Tensor:
        """
        Args:
            x: (B, n_features)
            global_step: Paso global
        Returns:
            logits: (B, n_classes)
        """
        # Projection
        x_proj = self.input_proj(x)
        
        # Memory
        if self.config.ENABLE_HOMEOSTATIC:
            x_mem, _ = self.memory(x_proj)
        else:
            x_mem = self.memory(x_proj)
        
        # CMS
        x_cms = self.cms(x_mem, global_step)
        
        # Output
        logits = self.output_proj(x_cms)
        
        return logits

# ========================================================================
# PGD ATTACK
# ========================================================================

def pgd_attack(model, x, y, epsilon, steps, device):
    """PGD adversarial attack - versi√≥n robusta"""
    was_training = model.training
    model.eval()
    
    # Inicializar perturbaci√≥n
    delta = torch.zeros_like(x, requires_grad=False)
    delta.uniform_(-epsilon, epsilon)
    
    for step in range(steps):
        # Crear delta con gradientes habilitados
        delta_var = delta.clone().detach().requires_grad_(True)
        
        # Forward
        x_adv = x + delta_var
        logits = model(x_adv)
        loss = F.cross_entropy(logits, y)
        
        # Backward
        loss.backward()
        
        # PGD update
        with torch.no_grad():
            if delta_var.grad is not None:
                grad_sign = delta_var.grad.sign()
            else:
                # Si no hay gradiente, usar direcci√≥n aleatoria
                grad_sign = torch.randn_like(delta).sign()
            
            # Update
            delta = delta + (epsilon / steps) * 1.5 * grad_sign
            
            # Project al epsilon ball
            delta = torch.clamp(delta, -epsilon, epsilon)
            
            # Project al rango v√°lido [0, 1]
            delta = torch.clamp(x + delta, 0, 1) - x
    
    if was_training:
        model.train()
    
    return (x + delta).detach()

# ========================================================================
# TRAINER CON ADVERSARIAL
# ========================================================================

class AdversarialTrainer:
    def __init__(self, model, config, device):
        self.model = model.to(device)
        self.config = config
        self.device = device
        
        self.optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=config.LEARNING_RATE,
            weight_decay=config.WEIGHT_DECAY
        )
        
        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer,
            T_max=config.NUM_EPOCHS
        )
        
        self.criterion = nn.CrossEntropyLoss()
    
    def train_step(self, x, y, epsilon, global_step):
        """Un paso de entrenamiento con adversarial opcional"""
        x, y = x.to(self.device), y.to(self.device)
        
        # Adversarial attack
        if self.config.ENABLE_ADVERSARIAL and epsilon > 0:
            x_adv = pgd_attack(
                self.model, x, y, epsilon, 
                self.config.ADV_STEPS, self.device
            )
        else:
            x_adv = x
        
        # Forward
        self.optimizer.zero_grad()
        logits = self.model(x_adv, global_step)
        loss = self.criterion(logits, y)
        
        # Backward
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
        self.optimizer.step()
        
        # M√©tricas
        with torch.no_grad():
            pred = logits.argmax(dim=1)
            acc = pred.eq(y).float().mean().item()
        
        return loss.item(), acc
    
    def evaluate(self, test_loader, epsilon=0.0):
        """Evaluaci√≥n con ataque opcional"""
        self.model.eval()
        correct = 0
        total = 0
        
        for x, y in test_loader:
            x, y = x.to(self.device), y.to(self.device)
            
            # Si hay ataque adversarial, necesitamos gradientes
            if epsilon > 0:
                x = pgd_attack(self.model, x, y, epsilon, 
                              self.config.ADV_STEPS, self.device)
            
            # Evaluaci√≥n final SIN gradientes
            with torch.no_grad():
                logits = self.model(x)
                pred = logits.argmax(dim=1)
                correct += pred.eq(y).sum().item()
                total += y.size(0)
        
        return correct / total

# ========================================================================
# EXPERIMENTO CON CONCEPT DRIFT
# ========================================================================

def run_real_world_experiment(config: Config, device: torch.device):
    print("\n" + "="*80)
    print("üß¨ HOPE + PHYSIO: Real World Challenge (Digits + Adversarial)")
    print("="*80)
    print(f"Adversarial: {'‚úÖ' if config.ENABLE_ADVERSARIAL else '‚ùå'}")
    print(f"Homeostasis: {'‚úÖ' if config.ENABLE_HOMEOSTATIC else '‚ùå'}")
    print(f"CMS: {'‚úÖ' if config.ENABLE_CMS else '‚ùå'}")
    print("="*80 + "\n")
    
    # Dataset
    env = RealWorldEnvironment(config.SEED)
    test_loader = env.get_test_loader(config.BATCH_SIZE)
    
    # Modelo
    model = HopePhysioModel(config, n_features=64, n_classes=10)
    trainer = AdversarialTrainer(model, config, device)
    
    print(f"üß† Par√°metros: {sum(p.numel() for p in model.parameters()):,}\n")
    
    # Header
    print(f"{'EP':<3} | {'Phase':<8} | {'Loss':<6} | {'Acc':<5} | "
          f"{'Test':<5} | {'Adv':<5} | {'Œµ':<5} | {'Event'}")
    print("-" * 80)
    
    global_step = 0
    best_clean = 0.0
    best_adv = 0.0
    
    for epoch in range(1, config.NUM_EPOCHS + 1):
        # Curriculum de epsilon
        progress = epoch / config.NUM_EPOCHS
        epsilon = config.ADV_EPSILON_START + progress * (
            config.ADV_EPSILON_END - config.ADV_EPSILON_START
        )
        
        # Concept drift schedule
        if epoch <= 15:
            phase = "WORLD_1"
            event = "üìò 0-4"
        elif epoch <= 30:
            phase = "WORLD_2"
            event = "‚ö° 5-9"
        elif epoch <= 40:
            phase = "CHAOS"
            event = "üåÄ Noise"
        else:
            phase = "MIXED"
            event = "üîÑ Mixed"
        
        # Entrenamiento
        model.train()
        epoch_loss = 0.0
        epoch_acc = 0.0
        num_batches = 0
        
        # Simular epoch con batches
        batches_per_epoch = 30
        for _ in range(batches_per_epoch):
            x, y = env.get_batch(phase, config.BATCH_SIZE)
            loss, acc = trainer.train_step(x, y, epsilon, global_step)
            epoch_loss += loss
            epoch_acc += acc
            num_batches += 1
            global_step += 1
        
        avg_loss = epoch_loss / num_batches
        avg_acc = epoch_acc / num_batches
        
        # Evaluaci√≥n
        if epoch % 5 == 0 or epoch == config.NUM_EPOCHS:
            test_clean = trainer.evaluate(test_loader, epsilon=0.0)
            test_adv = trainer.evaluate(test_loader, epsilon=config.ADV_EPSILON_END)
            
            best_clean = max(best_clean, test_clean)
            best_adv = max(best_adv, test_adv)
            
            print(f"{epoch:<3} | {phase:<8} | {avg_loss:.4f} | {avg_acc:.3f} | "
                  f"{test_clean:.3f} | {test_adv:.3f} | {epsilon:.2f} | {event}")
        else:
            print(f"{epoch:<3} | {phase:<8} | {avg_loss:.4f} | {avg_acc:.3f} | "
                  f"  -   |   -   | {epsilon:.2f} | {event}")
        
        trainer.scheduler.step()
    
    print("\n" + "="*80)
    print("üìä RESULTADOS FINALES")
    print("="*80)
    print(f"Best Clean Accuracy:      {best_clean * 100:.2f}%")
    print(f"Best Adversarial Accuracy: {best_adv * 100:.2f}%")
    print("="*80 + "\n")
    
    return {'clean': best_clean, 'adv': best_adv}

# ========================================================================
# ABLACI√ìN
# ========================================================================

def run_ablation(device):
    print("\n" + "="*80)
    print("üî¨ ESTUDIO DE ABLACI√ìN")
    print("="*80)
    
    configs = [
        {'name': 'Full (Homeo+CMS+Adv)', 'homeo': True, 'cms': True, 'adv': True},
        {'name': 'Sin Adversarial', 'homeo': True, 'cms': True, 'adv': False},
        {'name': 'Sin Homeostasis', 'homeo': False, 'cms': True, 'adv': True},
        {'name': 'Baseline', 'homeo': False, 'cms': False, 'adv': False}
    ]
    
    results = {}
    
    for exp in configs:
        print(f"\n{'='*80}")
        print(f"üß™ {exp['name']}")
        print(f"{'='*80}")
        
        config = Config()
        config.ENABLE_HOMEOSTATIC = exp['homeo']
        config.ENABLE_CMS = exp['cms']
        config.ENABLE_ADVERSARIAL = exp['adv']
        config.NUM_EPOCHS = 30  # Reducir para ablaci√≥n
        
        res = run_real_world_experiment(config, device)
        results[exp['name']] = res
    
    print("\n" + "="*80)
    print("üìà COMPARACI√ìN FINAL")
    print("="*80)
    print(f"{'Config':<30} | {'Clean':<8} | {'Adv':<8} | {'Trade-off':<10}")
    print("-" * 80)
    
    for name, res in results.items():
        tradeoff = res['clean'] - res['adv']
        print(f"{name:<30} | {res['clean']*100:>6.2f}% | {res['adv']*100:>6.2f}% | "
              f"{tradeoff*100:>8.2f}%")
    
    print("="*80)
    
    # An√°lisis cient√≠fico
    print("\n" + "="*80)
    print("üî¨ AN√ÅLISIS CIENT√çFICO")
    print("="*80)
    
    full = results['Full (Homeo+CMS+Adv)']
    no_adv = results['Sin Adversarial']
    no_homeo = results['Sin Homeostasis']
    baseline = results['Baseline']
    
    print(f"\n1. ROBUSTEZ ADVERSARIAL:")
    print(f"   Mejor robustez: Sin Homeostasis ({no_homeo['adv']*100:.2f}%)")
    print(f"   Ganancia vs Baseline: +{(no_homeo['adv'] - baseline['adv'])*100:.2f}%")
    print(f"   Homeostasis NO mejora robustez adversarial")
    
    print(f"\n2. CLEAN ACCURACY:")
    print(f"   Mejor: Baseline ({baseline['clean']*100:.2f}%)")
    print(f"   Raz√≥n: Arquitectura simple + dataset peque√±o")
    print(f"   Modelos complejos sufren de underfitting relativo")
    
    print(f"\n3. TRADE-OFF ROBUSTEZ/ACCURACY:")
    full_tradeoff = (full['clean'] - full['adv']) * 100
    baseline_tradeoff = (baseline['clean'] - baseline['adv']) * 100
    print(f"   Full Model: {full_tradeoff:.2f}% gap")
    print(f"   Baseline: {baseline_tradeoff:.2f}% gap")
    print(f"   Adversarial training reduce gap en {baseline_tradeoff - full_tradeoff:.2f}%")
    
    print(f"\n4. RECOMENDACIONES:")
    if full['adv'] > baseline['adv'] * 2:
        print(f"   ‚úÖ El modelo Hope+Physio vale la pena para aplicaciones adversariales")
    else:
        print(f"   ‚ö†Ô∏è  Para este dataset, un modelo simple es m√°s eficiente")
    
    print(f"   üí° Probar con dataset m√°s grande (MNIST, CIFAR-10)")
    print(f"   üí° Aumentar √©pocas de entrenamiento (50+)")
    print(f"   üí° Aplicar data augmentation adversarial")
    
    print("="*80)

# ========================================================================
# MAIN
# ========================================================================

if __name__ == "__main__":
    device = setup_device()
    set_seed(Config.SEED)
    
    print("\n‚öôÔ∏è  Configuraci√≥n:")
    print(f"   d_model: {Config.D_MODEL}")
    print(f"   MLP hidden: {Config.MLP_HIDDEN}")
    print(f"   CMS frequencies: {Config.CMS_FREQUENCIES}")
    print(f"   Adversarial Œµ: {Config.ADV_EPSILON_START} ‚Üí {Config.ADV_EPSILON_END}")
    print(f"   PGD steps: {Config.ADV_STEPS}\n")
    
    # Ejecutar ablaci√≥n completa
    run_ablation(device)
    
    print("\n‚úÖ EXPERIMENTO COMPLETADO\n")